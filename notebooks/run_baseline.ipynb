{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport matplotlib.pyplot as plt\nimport json\nimport re\nimport numpy as np\n\nfrom collections import defaultdict\nfrom PIL import Image\n\nrepo_dir = \"/kaggle/input/vqa-baseline\"\ntrain_dir = \"/kaggle/input/vqav2-train\"\nvalid_dir = \"/kaggle/input/vqav2-val\"","metadata":{"execution":{"iopub.status.busy":"2023-09-20T07:43:49.717125Z","iopub.execute_input":"2023-09-20T07:43:49.719148Z","iopub.status.idle":"2023-09-20T07:43:49.729385Z","shell.execute_reply.started":"2023-09-20T07:43:49.719074Z","shell.execute_reply":"2023-09-20T07:43:49.727854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing phase","metadata":{}},{"cell_type":"markdown","source":"## Resize images","metadata":{}},{"cell_type":"code","source":"def resize_image(image, size):\n    return image.resize(size, Image.Resampling.LANCZOS)\n\ndef resize_images(input_dir, output_dir, suffix, size):\n    for idir in os.scandir(input_dir):\n        if idir.name != suffix:\n            print(idir.name)\n            continue\n\n        if not os.path.exists(output_dir + '/' + idir.name):\n            os.makedirs(output_dir + '/' + idir.name)\n        \n        image_path = idir.path + '/' + suffix\n        images = os.listdir(image_path)\n        n_images = len(images)\n        for iimage, image in enumerate(images):\n            img = Image.open(os.path.join(image_path, image))\n            img = resize_image(img, size)\n            img.save(os.path.join(output_dir + '/' + idir.name, image), img.format)\n\n            if (iimage + 1) % 1000 == 0:\n                print(\"[{}/{}] Resized the images and saved into '{}'.\"\n                      .format(iimage+1, n_images, output_dir+'/'+idir.name))","metadata":{"execution":{"iopub.status.busy":"2023-09-19T17:14:25.021231Z","iopub.execute_input":"2023-09-19T17:14:25.021725Z","iopub.status.idle":"2023-09-19T17:14:25.034368Z","shell.execute_reply.started":"2023-09-19T17:14:25.021683Z","shell.execute_reply":"2023-09-19T17:14:25.032787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize_images(train_dir, \"/kaggle/working/Images\", \"train2014\", [224, 224])\nresize_images(valid_dir, \"/kaggle/working/Images\", \"val2014\", [224, 224])","metadata":{"execution":{"iopub.status.busy":"2023-09-19T17:19:06.805381Z","iopub.execute_input":"2023-09-19T17:19:06.805840Z","iopub.status.idle":"2023-09-19T18:05:53.120572Z","shell.execute_reply.started":"2023-09-19T17:19:06.805802Z","shell.execute_reply":"2023-09-19T18:05:53.118681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Vocab for QA","metadata":{}},{"cell_type":"code","source":"def make_vocab_questions(input_dirs):\n    \"\"\"Make dictionary for questions and save them into text file.\"\"\"\n    vocab_set = set()\n    SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n    question_length = []\n    datasets = [list(os.scandir(input_dirs[i]))[0] for i in range(len(input_dirs))]\n    for dataset in datasets:\n        with open(dataset.path) as f:\n            questions = json.load(f)['questions']\n        set_question_length = [None]*len(questions)\n        for iquestion, question in enumerate(questions):\n            words = SENTENCE_SPLIT_REGEX.split(question['question'].lower())\n            words = [w.strip() for w in words if len(w.strip()) > 0]\n            vocab_set.update(words)\n            set_question_length[iquestion] = len(words)\n        question_length += set_question_length\n\n    vocab_list = list(vocab_set)\n    vocab_list.sort()\n    vocab_list.insert(0, '<pad>')\n    vocab_list.insert(1, '<unk>')\n    \n    with open(r'/kaggle/working/vocab_questions.txt', 'w') as f:\n        f.writelines([w+'\\n' for w in vocab_list])\n    \n    print('Make vocabulary for questions')\n    print('The number of total words of questions: %d' % len(vocab_set))\n    print('Maximum length of question: %d' % np.max(question_length))\n\n\ndef make_vocab_answers(input_dirs, n_answers):\n    \"\"\"Make dictionary for top n answers and save them into text file.\"\"\"\n    answers = defaultdict(lambda: 0)\n    datasets = [list(os.scandir(input_dirs[i]))[0] for i in range(len(input_dirs))]\n    for dataset in datasets:\n        with open(dataset.path) as f:\n            annotations = json.load(f)['annotations']\n        for annotation in annotations:\n            for answer in annotation['answers']:\n                word = answer['answer']\n                if re.search(r\"[^\\w\\s]\", word):\n                    continue\n                answers[word] += 1\n                \n    answers = sorted(answers, key=answers.get, reverse=True)\n    assert('<unk>' not in answers)\n    top_answers = ['<unk>'] + answers[:n_answers-1] # '-1' is due to '<unk>'\n    \n    with open(r'/kaggle/working/vocab_answers.txt', 'w') as f:\n        f.writelines([w+'\\n' for w in top_answers])\n\n    print('Make vocabulary for answers')\n    print('The number of total words of answers: %d' % len(answers))\n    print('Keep top %d answers into vocab' % n_answers)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:53:16.351278Z","iopub.execute_input":"2023-09-20T06:53:16.351692Z","iopub.status.idle":"2023-09-20T06:53:16.370122Z","shell.execute_reply.started":"2023-09-20T06:53:16.351658Z","shell.execute_reply":"2023-09-20T06:53:16.368671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_vocab_questions(\n    ['/kaggle/input/vqav2-train/v2_Questions_Train_mscoco',\n     '/kaggle/input/vqav2-val/v2_Questions_Val_mscoco']\n)\n\nmake_vocab_answers(\n    ['/kaggle/input/vqav2-train/v2_Annotations_Train_mscoco',\n     '/kaggle/input/vqav2-val/v2_Annotations_Val_mscoco'],\n    n_answers=1000\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:53:55.499863Z","iopub.execute_input":"2023-09-20T06:53:55.500239Z","iopub.status.idle":"2023-09-20T06:54:38.324375Z","shell.execute_reply.started":"2023-09-20T06:53:55.500210Z","shell.execute_reply":"2023-09-20T06:54:38.323339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build VQA input","metadata":{}},{"cell_type":"code","source":"SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n\n\ndef tokenize(sentence):\n    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n    return tokens\n\n\ndef load_str_list(fname):\n    with open(fname) as f:\n        lines = f.readlines()\n    lines = [l.strip() for l in lines]\n    return lines\n\n\nclass VocabDict:\n    \n    def __init__(self, vocab_file):\n        self.word_list = load_str_list(vocab_file)\n        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n        self.vocab_size = len(self.word_list)\n        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n\n    def idx2word(self, n_w):\n        return self.word_list[n_w]\n\n    def word2idx(self, w):\n        if w in self.word2idx_dict:\n            return self.word2idx_dict[w]\n        elif self.unk2idx is not None:\n            return self.unk2idx\n        else:\n            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n\n    def tokenize_and_index(self, sentence):\n        inds = [self.word2idx(w) for w in tokenize(sentence)]\n        return inds","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:57:11.099901Z","iopub.execute_input":"2023-09-20T06:57:11.101797Z","iopub.status.idle":"2023-09-20T06:57:11.116581Z","shell.execute_reply.started":"2023-09-20T06:57:11.101738Z","shell.execute_reply":"2023-09-20T06:57:11.115584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_answers(q_answers, valid_answer_set):\n    all_answers = [answer[\"answer\"] for answer in q_answers]\n    valid_answers = [a for a in all_answers if a in valid_answer_set]\n    return all_answers, valid_answers\n\n\ndef vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, image_set):\n    print('building vqa %s dataset' % image_set)\n    if image_set in ['train2014', 'val2014']:\n        load_answer = True\n        with open(annotation_file.format(f'vqav2-{image_set[:-4]}', image_set[:-4].capitalize(), image_set)) as f:\n            annotations = json.load(f)['annotations']\n            qid2ann_dict = {ann['question_id']: ann for ann in annotations}\n    else:\n        load_answer = False\n    with open(question_file.format(f'vqav2-{image_set[:-4]}', image_set[:-4].capitalize(), image_set)) as f:\n        questions = json.load(f)['questions']\n    coco_set_name = image_set.replace('-dev', '')\n    abs_image_dir = os.path.abspath(image_dir % coco_set_name)\n    image_name_template = 'COCO_'+coco_set_name+'_%012d'\n    dataset = [None]*len(questions)\n    \n    unk_ans_count = 0\n    for n_q, q in enumerate(questions):\n        if (n_q+1) % 10000 == 0:\n            print('processing %d / %d' % (n_q+1, len(questions)))\n        image_id = q['image_id']\n        question_id = q['question_id']\n        image_name = image_name_template % image_id\n        image_path = os.path.join(abs_image_dir, image_name+'.jpg')\n        question_str = q['question']\n        question_tokens = tokenize(question_str)\n        \n        iminfo = dict(image_name=image_name,\n                      image_path=image_path,\n                      question_id=question_id,\n                      question_str=question_str,\n                      question_tokens=question_tokens)\n        \n        if load_answer:\n            ann = qid2ann_dict[question_id]\n            all_answers, valid_answers = extract_answers(ann['answers'], valid_answer_set)\n            if len(valid_answers) == 0:\n                valid_answers = ['<unk>']\n                unk_ans_count += 1\n            iminfo['all_answers'] = all_answers\n            iminfo['valid_answers'] = valid_answers\n            \n        dataset[n_q] = iminfo\n    print('total %d out of %d answers are <unk>' % (unk_ans_count, len(questions)))\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-20T07:21:07.237466Z","iopub.execute_input":"2023-09-20T07:21:07.238198Z","iopub.status.idle":"2023-09-20T07:21:07.253175Z","shell.execute_reply.started":"2023-09-20T07:21:07.238162Z","shell.execute_reply":"2023-09-20T07:21:07.251976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_dir = '/kaggle/working/Images/%s/'\n\nannotation_file = '/kaggle/input/{}/v2_Annotations_{}_mscoco/v2_mscoco_{}_annotations.json'\nquestion_file = '/kaggle/input/{}/v2_Questions_{}_mscoco/v2_OpenEnded_mscoco_{}_questions.json'\n\nvocab_answer_file = '/kaggle/working/vocab_answers.txt'\nanswer_dict = VocabDict(vocab_answer_file)\nvalid_answer_set = set(answer_dict.word_list)\n\ntrain = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'train2014')\nvalid = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'val2014')\n# test = vqa_processing(image_dir, annotation_file, question_file, valid_answer_set, 'test2015')\n\nnp.save('/kaggle/working/train.npy', np.array(train))\nnp.save('/kaggle/working/valid.npy', np.array(valid))\n# np.save(args.output_dir+'/test.npy', np.array(test))","metadata":{"execution":{"iopub.status.busy":"2023-09-20T07:26:09.539185Z","iopub.execute_input":"2023-09-20T07:26:09.540217Z","iopub.status.idle":"2023-09-20T07:27:02.659936Z","shell.execute_reply.started":"2023-09-20T07:26:09.540167Z","shell.execute_reply":"2023-09-20T07:27:02.658701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Phase","metadata":{}},{"cell_type":"code","source":"%cd {repo_dir}\n!ls","metadata":{"execution":{"iopub.status.busy":"2023-09-20T07:44:01.797618Z","iopub.execute_input":"2023-09-20T07:44:01.798708Z","iopub.status.idle":"2023-09-20T07:44:03.001737Z","shell.execute_reply.started":"2023-09-20T07:44:01.798670Z","shell.execute_reply":"2023-09-20T07:44:03.000355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py --input /kaggle/working/ --log_dir /kaggle/working/ --model_dir /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-09-20T07:44:09.339323Z","iopub.execute_input":"2023-09-20T07:44:09.339752Z","iopub.status.idle":"2023-09-20T08:17:34.541612Z","shell.execute_reply.started":"2023-09-20T07:44:09.339713Z","shell.execute_reply":"2023-09-20T08:17:34.540388Z"},"trusted":true},"execution_count":null,"outputs":[]}]}