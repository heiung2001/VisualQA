{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!wget https://huggingface.co/spaces/CVPR/VizWiz-CLIP-VQA/raw/main/data/annotations/class_mapping.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:58:54.304793Z","iopub.status.busy":"2023-09-25T03:58:54.303877Z","iopub.status.idle":"2023-09-25T03:59:05.325703Z","shell.execute_reply":"2023-09-25T03:59:05.324302Z","shell.execute_reply.started":"2023-09-25T03:58:54.304727Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","import csv\n","import numpy as np\n","import random\n","import re\n","from typing import Optional\n","from tqdm import tqdm\n","import PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\n","from PIL import Image\n","import torch\n","from torch import Tensor\n","from torch.utils.data import DataLoader, Dataset\n","from torch import optim, nn\n","from transformers import ViltProcessor, ViltForQuestionAnswering"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:59:05.328323Z","iopub.status.busy":"2023-09-25T03:59:05.327906Z","iopub.status.idle":"2023-09-25T03:59:05.336140Z","shell.execute_reply":"2023-09-25T03:59:05.334571Z","shell.execute_reply.started":"2023-09-25T03:59:05.328288Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using\", device)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:59:05.338710Z","iopub.status.busy":"2023-09-25T03:59:05.338218Z","iopub.status.idle":"2023-09-25T03:59:05.353717Z","shell.execute_reply":"2023-09-25T03:59:05.352160Z","shell.execute_reply.started":"2023-09-25T03:59:05.338664Z"},"trusted":true},"outputs":[],"source":["train_json_dir = \"/kaggle/input/vizwiz-2023-edition/Annotations/train.json\"\n","val_json_dir = \"/kaggle/input/vizwiz-2023-edition/Annotations/val.json\"\n","classmapping_dir = \"class_mapping.csv\"\n","\n","batch_size = 32\n","num_epoch = 25\n","learning_rate = 5e-4\n","es_patience = 8\n","lr_patience = 3\n","model_save_path = \"ckpt_vilt.pth\""]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:59:05.358722Z","iopub.status.busy":"2023-09-25T03:59:05.357642Z","iopub.status.idle":"2023-09-25T03:59:05.381951Z","shell.execute_reply":"2023-09-25T03:59:05.380600Z","shell.execute_reply.started":"2023-09-25T03:59:05.358659Z"},"trusted":true},"outputs":[],"source":["with open(classmapping_dir, \"r\") as f:\n","    next(f)  # Skip the header\n","    reader = csv.reader(f, skipinitialspace=True)\n","    class_mapping = dict(reader)\n","    label2id = {k: int(v) for k, v in class_mapping.items()}\n","    id2label = {v: k for k, v in label2id.items()}"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:59:05.384798Z","iopub.status.busy":"2023-09-25T03:59:05.383621Z","iopub.status.idle":"2023-09-25T03:59:05.395067Z","shell.execute_reply":"2023-09-25T03:59:05.393746Z","shell.execute_reply.started":"2023-09-25T03:59:05.384739Z"},"trusted":true},"outputs":[],"source":["def get_score(count: int) -> float:\n","    return min(1.0, count / 3)\n","\n","def add_label_score(annotations):\n","    for annotation in tqdm(annotations):\n","        answers_dict = annotation[\"answers\"]\n","        answer_count = {}\n","        for answers in answers_dict:\n","            answer = answers[\"answer\"]\n","            answer_count[answer] = answer_count.get(answer, 0) + 1\n","\n","        labels = []\n","        scores = []\n","        for answer_word in answer_count:\n","            if answer_word in list(label2id.keys()):\n","                labels.append(label2id[answer_word])\n","                scores.append(get_score(answer_count[answer_word]))\n","        annotation[\"labels\"] = labels\n","        annotation[\"scores\"] = scores"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ShearX(img, v):  # [-0.3, 0.3]\n","    assert -0.3 <= v <= 0.3\n","    if random.random() > 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n","\n","\n","def ShearY(img, v):  # [-0.3, 0.3]\n","    assert -0.3 <= v <= 0.3\n","    if random.random() > 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n","\n","\n","def TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n","    assert -0.45 <= v <= 0.45\n","    if random.random() > 0.5:\n","        v = -v\n","    v = v * img.size[0]\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n","\n","\n","def TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n","    assert 0 <= v\n","    if random.random() > 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n","\n","\n","def TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n","    assert -0.45 <= v <= 0.45\n","    if random.random() > 0.5:\n","        v = -v\n","    v = v * img.size[1]\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n","\n","\n","def TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n","    assert 0 <= v\n","    if random.random() > 0.5:\n","        v = -v\n","    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n","\n","\n","def Rotate(img, v):  # [-30, 30]\n","    assert -30 <= v <= 30\n","    if random.random() > 0.5:\n","        v = -v\n","    return img.rotate(v)\n","\n","\n","def AutoContrast(img, _):\n","    return PIL.ImageOps.autocontrast(img)\n","\n","\n","def Equalize(img, _):\n","    return PIL.ImageOps.equalize(img)\n","\n","\n","def Flip(img, _):  # not from the paper\n","    return PIL.ImageOps.mirror(img)\n","\n","\n","def Solarize(img, v):  # [0, 256]\n","    assert 0 <= v <= 256\n","    return PIL.ImageOps.solarize(img, v)\n","\n","\n","def SolarizeAdd(img, addition=0, threshold=128):\n","    img_np = np.array(img).astype(np.int32)\n","    img_np = img_np + addition\n","    img_np = np.clip(img_np, 0, 255)\n","    img_np = img_np.astype(np.uint8)\n","    img = Image.fromarray(img_np)\n","    return PIL.ImageOps.solarize(img, threshold)\n","\n","\n","def Posterize(img, v):  # [4, 8]\n","    v = int(v)\n","    v = max(1, v)\n","    return PIL.ImageOps.posterize(img, v)\n","\n","\n","def Contrast(img, v):  # [0.1,1.9]\n","    assert 0.1 <= v <= 1.9\n","    return PIL.ImageEnhance.Contrast(img).enhance(v)\n","\n","\n","def Color(img, v):  # [0.1,1.9]\n","    assert 0.1 <= v <= 1.9\n","    return PIL.ImageEnhance.Color(img).enhance(v)\n","\n","\n","def Brightness(img, v):  # [0.1,1.9]\n","    assert 0.1 <= v <= 1.9\n","    return PIL.ImageEnhance.Brightness(img).enhance(v)\n","\n","\n","def Sharpness(img, v):  # [0.1,1.9]\n","    assert 0.1 <= v <= 1.9\n","    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n","\n","\n","def SamplePairing(imgs):  # [0, 0.4]\n","    def f(img1, v):\n","        i = np.random.choice(len(imgs))\n","        img2 = PIL.Image.fromarray(imgs[i])\n","        return PIL.Image.blend(img1, img2, v)\n","\n","    return f\n","\n","\n","def Identity(img, v):\n","    return img\n","\n","\n","def augment_list():  # 16 oeprations and their ranges\n","    # https://github.com/google-research/uda/blob/master/image/randaugment/policies.py#L57\n","    # https://github.com/tensorflow/tpu/blob/8462d083dd89489a79e3200bcc8d4063bf362186/models/official/efficientnet/autoaugment.py#L505\n","    l = [\n","        (AutoContrast, 0, 1),\n","        (Equalize, 0, 1),\n","        (Rotate, 0, 30),\n","        (Posterize, 0, 4),\n","        (Color, 0.1, 1.9),\n","        (Contrast, 0.1, 1.9),\n","        (Brightness, 0.1, 1.9),\n","        (Sharpness, 0.1, 1.9),\n","        (ShearX, 0.0, 0.3),\n","        (ShearY, 0.0, 0.3),\n","        (TranslateXabs, 0.0, 100),\n","        (TranslateYabs, 0.0, 100),\n","    ]\n","\n","    return l\n","\n","\n","class RandAugment:\n","    def __init__(self, n, m):\n","        self.n = n\n","        self.m = m  # [0, 30]\n","        self.augment_list = augment_list()\n","\n","    def __call__(self, img):\n","        ops = random.choices(self.augment_list, k=self.n)\n","        for op, minval, maxval in ops:\n","            val = (float(self.m) / 30) * float(maxval - minval) + minval\n","            img = op(img, val)\n","\n","        return img"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:59:05.397551Z","iopub.status.busy":"2023-09-25T03:59:05.397028Z","iopub.status.idle":"2023-09-25T03:59:05.413754Z","shell.execute_reply":"2023-09-25T03:59:05.412455Z","shell.execute_reply.started":"2023-09-25T03:59:05.397513Z"},"trusted":true},"outputs":[],"source":["class VQADataset(torch.utils.data.Dataset):\n","    \"\"\"VQA (v2) dataset.\"\"\"\n","\n","    def __init__(self, annotations, subset, processor):\n","        self.annotations = annotations\n","        self.processor = processor\n","        self.subset = subset\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def __getitem__(self, idx):\n","        # get image + text\n","        annotation = self.annotations[idx]\n","        image = Image.open(os.path.join(\"/kaggle/input/vizwiz-2023-edition\", self.subset, self.subset, annotation[\"image\"]))\n","        augmenter = RandAugment(n=2, m=9)\n","        image = augmenter(image)\n","        text = annotation['question']\n","\n","        encoding = self.processor(image, text, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","        # remove batch dimension\n","        for k,v in encoding.items():\n","            encoding[k] = v.squeeze()\n","        # add labels\n","        labels = annotation[\"labels\"]\n","        scores = annotation[\"scores\"]\n","        # # based on: https://github.com/dandelin/ViLT/blob/762fd3975c180db6fc88f577cf39549983fa373a/vilt/modules/objectives.py#L301\n","        targets = torch.zeros(len(label2id))\n","        for label, score in zip(labels, scores):\n","              targets[label] = score\n","        encoding[\"labels\"] = targets\n","\n","        return encoding"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:59:05.416050Z","iopub.status.busy":"2023-09-25T03:59:05.415582Z","iopub.status.idle":"2023-09-25T03:59:05.805023Z","shell.execute_reply":"2023-09-25T03:59:05.804025Z","shell.execute_reply.started":"2023-09-25T03:59:05.416014Z"},"trusted":true},"outputs":[],"source":["processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T03:59:05.807476Z","iopub.status.busy":"2023-09-25T03:59:05.806952Z","iopub.status.idle":"2023-09-25T03:59:24.687093Z","shell.execute_reply":"2023-09-25T03:59:24.685805Z","shell.execute_reply.started":"2023-09-25T03:59:05.807430Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 20523/20523 [00:14<00:00, 1370.07it/s]\n","100%|██████████| 4319/4319 [00:03<00:00, 1369.47it/s]\n"]}],"source":["with open(train_json_dir, \"r\") as f:\n","    train_data = json.load(f)\n","    add_label_score(train_data)\n","train_dataset = VQADataset(annotations=train_data, subset=\"train\", processor=processor)\n","\n","with open(val_json_dir, \"r\") as f:\n","    val_data = json.load(f)\n","    add_label_score(val_data)\n","val_dataset = VQADataset(annotations=val_data, subset=\"val\", processor=processor)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T04:05:07.562479Z","iopub.status.busy":"2023-09-25T04:05:07.561929Z","iopub.status.idle":"2023-09-25T04:05:10.553475Z","shell.execute_reply":"2023-09-25T04:05:10.551222Z","shell.execute_reply.started":"2023-09-25T04:05:07.562440Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-finetuned-vqa and are newly initialized because the shapes did not match:\n","- classifier.3.weight: found shape torch.Size([3129, 1536]) in the checkpoint and torch.Size([5726, 1536]) in the model instantiated\n","- classifier.3.bias: found shape torch.Size([3129]) in the checkpoint and torch.Size([5726]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["ViltForQuestionAnswering(\n","  (vilt): ViltModel(\n","    (embeddings): ViltEmbeddings(\n","      (text_embeddings): TextEmbeddings(\n","        (word_embeddings): Embedding(30522, 768)\n","        (position_embeddings): Embedding(40, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (patch_embeddings): ViltPatchEmbeddings(\n","        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n","      )\n","      (token_type_embeddings): Embedding(2, 768)\n","      (dropout): Dropout(p=0.0, inplace=False)\n","    )\n","    (encoder): ViltEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x ViltLayer(\n","          (attention): ViltAttention(\n","            (attention): ViltSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (output): ViltSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (intermediate): ViltIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ViltOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.0, inplace=False)\n","          )\n","          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (pooler): ViltPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=768, out_features=1536, bias=True)\n","    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n","    (2): GELU(approximate='none')\n","    (3): Linear(in_features=1536, out_features=5726, bias=True)\n","  )\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",\n","                                                 id2label=id2label,\n","                                                 label2id=label2id)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T04:05:28.156891Z","iopub.status.busy":"2023-09-25T04:05:28.156296Z","iopub.status.idle":"2023-09-25T04:05:28.171705Z","shell.execute_reply":"2023-09-25T04:05:28.169500Z","shell.execute_reply.started":"2023-09-25T04:05:28.156847Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    input_ids = [item['input_ids'] for item in batch]\n","    pixel_values = [item['pixel_values'] for item in batch]\n","    attention_mask = [item['attention_mask'] for item in batch]\n","    token_type_ids = [item['token_type_ids'] for item in batch]\n","    labels = [item['labels'] for item in batch]\n","\n","    # create padded pixel values and corresponding pixel mask\n","    encoding = processor.image_processor.pad(pixel_values, return_tensors=\"pt\")\n","\n","    # create new batch\n","    batch = {}\n","    batch['input_ids'] = torch.stack(input_ids)\n","    batch['attention_mask'] = torch.stack(attention_mask)\n","    batch['token_type_ids'] = torch.stack(token_type_ids)\n","    batch['pixel_values'] = encoding['pixel_values']\n","    batch['pixel_mask'] = encoding['pixel_mask']\n","    batch['labels'] = torch.stack(labels)\n","\n","    return batch\n","\n","\n","train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, \n","                              shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=1, \n","                            shuffle=False, num_workers=2, pin_memory=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T04:05:28.780883Z","iopub.status.busy":"2023-09-25T04:05:28.780309Z","iopub.status.idle":"2023-09-25T04:05:28.793680Z","shell.execute_reply":"2023-09-25T04:05:28.792051Z","shell.execute_reply.started":"2023-09-25T04:05:28.780842Z"},"trusted":true},"outputs":[],"source":["# Early Stopping\n","# Stop training if validation loss does not improve\n","class EarlyStopping:\n","\n","    def __init__(self, patience, model_save_path, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.model_save_path = model_save_path\n","        self.counter = 0\n","        self.min_validation_loss = np.inf\n","        self.best_epoch = 0\n","        self.early_stop = False\n","\n","\n","    def __call__(self, epoch, model, validation_loss):\n","        delta_loss = self.min_validation_loss - validation_loss\n","        # Check if val loss is smaller than min loss\n","        if delta_loss > self.min_delta:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","            # Save best model\n","            self.best_epoch = epoch\n","            torch.save(model.state_dict(), self.model_save_path)\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                print(f\"Early Stopping.\")\n","                print(f\"Save best model at epoch {self.best_epoch}\")\n","                self.early_stop = True"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T04:05:29.502679Z","iopub.status.busy":"2023-09-25T04:05:29.502022Z","iopub.status.idle":"2023-09-25T04:05:29.513148Z","shell.execute_reply":"2023-09-25T04:05:29.511748Z","shell.execute_reply.started":"2023-09-25T04:05:29.502629Z"},"trusted":true},"outputs":[],"source":["# Define optimizer\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","# Reduce learning rate when validation loss stops improving\n","lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=lr_patience, verbose=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T04:05:29.873469Z","iopub.status.busy":"2023-09-25T04:05:29.872868Z","iopub.status.idle":"2023-09-25T04:05:29.889311Z","shell.execute_reply":"2023-09-25T04:05:29.887662Z","shell.execute_reply.started":"2023-09-25T04:05:29.873423Z"},"trusted":true},"outputs":[],"source":["def train_model(data_loader, model, optimizer, device):\n","    num_batches = len(data_loader)\n","    # Define loss\n","    total_loss = 0\n","    \n","    model.train()\n","    for batch in data_loader:\n","        # get the inputs;\n","        inputs = {k:v.to(device) for k,v in batch.items()}\n","        # forward pass\n","        outputs = model(**inputs)\n","        loss = outputs.loss\n","        # backward and optimize\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        \n","    # Loss over batches\n","    train_loss = total_loss / num_batches\n","\n","    return train_loss\n","\n","\n","def val_model(data_loader, model, device):\n","    num_batches = len(data_loader)\n","    # Define loss and accuracy\n","    total_loss = 0\n","    total_acc = 0\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            inputs = {k:v.to(device) for k,v in batch.items()}\n","            outputs = model(**inputs)\n","\n","            # Calculate loss\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            # Get top predict for each question\n","            preds = outputs.logits.argmax(-1)\n","            # Get ground truth answers for each questiojn\n","            scores, labels = batch[\"labels\"].to(device).topk(10, -1)\n","            # Calculate accuracy\n","            total_acc += scores[preds==labels].sum() / len(preds)\n","            \n","    # Accuracy over batches\n","    val_acc = total_acc / num_batches\n","    # Loss over batches\n","    val_loss = total_loss / num_batches\n","\n","    return val_acc, val_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-25T04:05:30.342230Z","iopub.status.busy":"2023-09-25T04:05:30.341652Z","iopub.status.idle":"2023-09-25T04:08:00.321636Z","shell.execute_reply":"2023-09-25T04:08:00.319080Z","shell.execute_reply.started":"2023-09-25T04:05:30.342185Z"},"trusted":true},"outputs":[],"source":["# Log metrics for plotting\n","all_losses = []\n","\n","# Initialize Early Stopping object\n","early_stopper = EarlyStopping(patience=es_patience, model_save_path=model_save_path)\n","for epoch in range(num_epoch):\n","    print(f\"Epoch [{epoch}/{num_epoch-1}]\")\n","    train_loss = train_model(train_dataloader, model, optimizer, device)\n","    val_acc, val_loss = val_model(val_dataloader, model, device)\n","    \n","    all_losses.append([train_loss, val_loss])\n","\n","    # Display\n","    print(f\"Train loss: {train_loss:.5f} - Val loss: {val_loss:.5f}\")\n","    print(f\"Val accuracy: {val_acc:.5f}\\n\")\n","    \n","    # EarlyStopping\n","    early_stopper(epoch, model, val_loss)\n","    if early_stopper.early_stop:\n","        break\n","    # Adjust learning rate\n","    lr_scheduler.step(val_loss)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
